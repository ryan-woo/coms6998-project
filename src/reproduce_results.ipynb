{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2164b9df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/amith/Documents/columbia/phd/2021-f/computation_and_the_brain/coms6998-project\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20a2b664",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amith/Documents/columbia/phd/2021-f/computation_and_the_brain/coms6998-project/env/lib/python3.8/site-packages/brainscore/metrics/__init__.py:37: FutureWarning: xarray subclass Score should explicitly define __slots__\n",
      "  class Score(DataAssembly):\n"
     ]
    }
   ],
   "source": [
    "from neural_nlp.benchmarks import benchmark_pool\n",
    "pereira = benchmark_pool[\"Pereira2018-encoding\"]\n",
    "data = pereira._load_assembly(version='base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7c23bc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              sentence  sentence_num  \\\n",
      "0    Beekeeping encourages the conservation of loca...             0   \n",
      "1    It is in every beekeeper's interest to conserv...             1   \n",
      "2    As a passive form of agriculture, it does not ...             2   \n",
      "3    Beekeepers also discourage the use of pesticid...             3   \n",
      "4    Artisanal beekeepers go to extremes for their ...             4   \n",
      "..                                                 ...           ...   \n",
      "622  Some windows have multiple panes to increase i...           379   \n",
      "623                   A woman is a female human adult.           380   \n",
      "624    A woman is stereotypically seen as a caregiver.           381   \n",
      "625     A woman can become pregnant and bear children.           382   \n",
      "626  A woman has different reproductive organs than...           383   \n",
      "\n",
      "          stimulus_id    experiment                       story  \\\n",
      "0      243sentences.0  243sentences     243sentences.beekeeping   \n",
      "1      243sentences.1  243sentences     243sentences.beekeeping   \n",
      "2      243sentences.2  243sentences     243sentences.beekeeping   \n",
      "3      243sentences.3  243sentences     243sentences.beekeeping   \n",
      "4      243sentences.4  243sentences     243sentences.beekeeping   \n",
      "..                ...           ...                         ...   \n",
      "622  384sentences.379  384sentences  384sentences.building_part   \n",
      "623  384sentences.380  384sentences          384sentences.human   \n",
      "624  384sentences.381  384sentences          384sentences.human   \n",
      "625  384sentences.382  384sentences          384sentences.human   \n",
      "626  384sentences.383  384sentences          384sentences.human   \n",
      "\n",
      "     passage_index passage_label passage_category      passage_id  \n",
      "0                1    beekeeping       beekeeping   243sentences1  \n",
      "1                1    beekeeping       beekeeping   243sentences1  \n",
      "2                1    beekeeping       beekeeping   243sentences1  \n",
      "3                1    beekeeping       beekeeping   243sentences1  \n",
      "4                2    beekeeping       beekeeping   243sentences2  \n",
      "..             ...           ...              ...             ...  \n",
      "622             95        Window    building_part  384sentences95  \n",
      "623             96         Woman            human  384sentences96  \n",
      "624             96         Woman            human  384sentences96  \n",
      "625             96         Woman            human  384sentences96  \n",
      "626             96         Woman            human  384sentences96  \n",
      "\n",
      "[627 rows x 9 columns]\n",
      "Counter({'243sentences1': 4, '243sentences2': 4, '243sentences4': 4, '243sentences7': 4, '243sentences10': 4, '243sentences14': 4, '243sentences16': 4, '243sentences19': 4, '243sentences22': 4, '243sentences25': 4, '243sentences28': 4, '243sentences31': 4, '243sentences34': 4, '243sentences37': 4, '243sentences43': 4, '243sentences44': 4, '243sentences46': 4, '243sentences49': 4, '243sentences51': 4, '243sentences52': 4, '243sentences55': 4, '243sentences58': 4, '243sentences61': 4, '243sentences64': 4, '243sentences68': 4, '243sentences69': 4, '243sentences70': 4, '384sentences1': 4, '384sentences2': 4, '384sentences3': 4, '384sentences4': 4, '384sentences5': 4, '384sentences6': 4, '384sentences7': 4, '384sentences8': 4, '384sentences9': 4, '384sentences10': 4, '384sentences11': 4, '384sentences12': 4, '384sentences13': 4, '384sentences14': 4, '384sentences15': 4, '384sentences16': 4, '384sentences17': 4, '384sentences18': 4, '384sentences19': 4, '384sentences20': 4, '384sentences21': 4, '384sentences22': 4, '384sentences23': 4, '384sentences24': 4, '384sentences25': 4, '384sentences26': 4, '384sentences27': 4, '384sentences28': 4, '384sentences29': 4, '384sentences30': 4, '384sentences31': 4, '384sentences32': 4, '384sentences33': 4, '384sentences34': 4, '384sentences35': 4, '384sentences36': 4, '384sentences37': 4, '384sentences38': 4, '384sentences39': 4, '384sentences40': 4, '384sentences41': 4, '384sentences42': 4, '384sentences43': 4, '384sentences44': 4, '384sentences45': 4, '384sentences46': 4, '384sentences47': 4, '384sentences48': 4, '384sentences49': 4, '384sentences50': 4, '384sentences51': 4, '384sentences52': 4, '384sentences53': 4, '384sentences54': 4, '384sentences55': 4, '384sentences56': 4, '384sentences57': 4, '384sentences58': 4, '384sentences59': 4, '384sentences60': 4, '384sentences61': 4, '384sentences62': 4, '384sentences63': 4, '384sentences64': 4, '384sentences65': 4, '384sentences66': 4, '384sentences67': 4, '384sentences68': 4, '384sentences69': 4, '384sentences70': 4, '384sentences71': 4, '384sentences72': 4, '384sentences73': 4, '384sentences74': 4, '384sentences75': 4, '384sentences76': 4, '384sentences77': 4, '384sentences78': 4, '384sentences79': 4, '384sentences80': 4, '384sentences81': 4, '384sentences82': 4, '384sentences83': 4, '384sentences84': 4, '384sentences85': 4, '384sentences86': 4, '384sentences87': 4, '384sentences88': 4, '384sentences89': 4, '384sentences90': 4, '384sentences91': 4, '384sentences92': 4, '384sentences93': 4, '384sentences94': 4, '384sentences95': 4, '384sentences96': 4, '243sentences3': 3, '243sentences5': 3, '243sentences6': 3, '243sentences8': 3, '243sentences9': 3, '243sentences11': 3, '243sentences12': 3, '243sentences13': 3, '243sentences15': 3, '243sentences17': 3, '243sentences18': 3, '243sentences20': 3, '243sentences21': 3, '243sentences23': 3, '243sentences24': 3, '243sentences26': 3, '243sentences27': 3, '243sentences29': 3, '243sentences30': 3, '243sentences32': 3, '243sentences33': 3, '243sentences35': 3, '243sentences36': 3, '243sentences38': 3, '243sentences39': 3, '243sentences40': 3, '243sentences41': 3, '243sentences42': 3, '243sentences45': 3, '243sentences47': 3, '243sentences48': 3, '243sentences50': 3, '243sentences53': 3, '243sentences54': 3, '243sentences56': 3, '243sentences57': 3, '243sentences59': 3, '243sentences60': 3, '243sentences62': 3, '243sentences63': 3, '243sentences65': 3, '243sentences66': 3, '243sentences67': 3, '243sentences71': 3, '243sentences72': 3})\n"
     ]
    }
   ],
   "source": [
    "# here are the various stimuli (passages with their accompanying sentences) which \n",
    "# were presented to the human participants in the experiment\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# from _PereiraBenchmark#call\n",
    "# we add a new passage identifier (experiment + the index of the passage read)\n",
    "# this will allow us to process each stimulus together (passage by passage)\n",
    "\n",
    "stimulus_set = data.attrs['stimulus_set']\n",
    "stimulus_set.loc[:, 'passage_id'] = stimulus_set['experiment'] + stimulus_set['passage_index'].astype(str)\n",
    "\n",
    "print(stimulus_set)\n",
    "print(Counter(stimulus_set['passage_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc5b1620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2TokenizerFast, GPT2Model\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2', output_hidden_states=True)\n",
    "model = model.eval()  \n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "18937ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 168/168 [00:24<00:00,  6.96it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# now we run the stimuli through our model and get their corresponding activations\n",
    "# we do so for each \"story\" (identified by passage_id) separately by concatenating\n",
    "# it's constituent sentences, keeping track of each sentences start and end tokens\n",
    "# to be able to retrieve their token representans\n",
    "\n",
    "# from stimulus_id -> 13 x 768 tensor (final representations from each layer)\n",
    "activations = {}\n",
    "for story in tqdm(sorted(set(stimulus_set['passage_id'].values))):\n",
    "    story_stimuli = stimulus_set[stimulus_set['passage_id'] == story]\n",
    "    \n",
    "    sentences = []\n",
    "    stimulus_ids = []\n",
    "    stimulus_ends = []\n",
    "    length_so_far = 0\n",
    "    for _, stimulus in story_stimuli.sort_values(by='sentence_num', ascending=True).iterrows():\n",
    "        length_so_far += len(stimulus['sentence'])\n",
    "        sentences.append(stimulus['sentence'])\n",
    "        stimulus_ids.append(stimulus['stimulus_id'])\n",
    "        stimulus_ends.append(length_so_far - 1)\n",
    "        \n",
    "        # we'll join the sentences with spaces \n",
    "        length_so_far += 1\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        tokenized = tokenizer(\n",
    "            [' '.join(sentences)], \n",
    "            add_special_tokens=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # note that the ending character here is usually a period \n",
    "        # (we can experiment w/ the last word by subtracting 1)\n",
    "        stimulus_token_ends = [\n",
    "            tokenized.char_to_token(stimulus_end) for stimulus_end in stimulus_ends\n",
    "        ]\n",
    "        \n",
    "        output = model(**tokenized)\n",
    "        \n",
    "        for stimulus_id, stimulus_token_end in zip(stimulus_ids, stimulus_token_ends):\n",
    "            assert stimulus_id not in activations\n",
    "            \n",
    "            # get hidden state of each final token for each stimulus\n",
    "            \n",
    "            activations[stimulus_id] = torch.stack([\n",
    "                output.hidden_states[i][0][stimulus_token_end] for i in range(len(output.hidden_states))\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f60322ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('cache/gpt2_activations.pkl', 'wb') as f:\n",
    "    pickle.dump(activations, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d2ca60d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('cache/gpt2_activations.pkl', 'rb') as f:\n",
    "    activations = pickle.load(f)\n",
    "    \n",
    "# sanity check that we have gpt activations for every stimulus\n",
    "    \n",
    "assert set(activations.keys()) == set(data['stimulus_id'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6801e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(627, 103900)\n",
      "627 Counter({'384sentences': 384, '243sentences': 243})\n",
      "103900 Counter({'visual': 43741, 'MD': 29936, 'language': 13553, 'DMN': 10978, 'auditory': 5692})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "627it [00:31, 19.86it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# now we have to split / group the data as done in the neural_nlp repo\n",
    "\n",
    "# here are our raw voxels\n",
    "print(data.values.shape)\n",
    "\n",
    "# for each subject in our data (dim 0), here are their corresponding experiments\n",
    "experiment_counts = Counter(data['experiment'].values)\n",
    "print(len(data['experiment']), experiment_counts)\n",
    "\n",
    "# for each voxel in our data (dim 1), here is its corresponding brain region (atlas)\n",
    "print(len(data['atlas']), Counter(data['atlas'].values))\n",
    "\n",
    "# we split the data by experiment and atlas (this is very slow...)\n",
    "# from brainscore.metrics.transformations import CartesianProduct\n",
    "# splitter = CartesianProduct(dividers=['experiment', 'atlas'])\n",
    "# splits = splitter(data, apply=lambda split: split.drop_vars(['experiment', 'atlas']))\n",
    "\n",
    "experiment_voxels = defaultdict(list)\n",
    "experiment_voxel_nas = defaultdict(set)\n",
    "experiment_subjects = defaultdict(list)\n",
    "experiment_stimuli = defaultdict(list)\n",
    "for subject_id, subject, stimulus_id, experiment in tqdm(zip(\n",
    "    range(data.shape[0]), \n",
    "    data['subject'].values, \n",
    "    data['stimulus_id'].values, \n",
    "    data['experiment'].values\n",
    ")):\n",
    "    subject_voxels = []\n",
    "    for voxel_id, atlas in zip(range(data.shape[1]), data['atlas'].values):\n",
    "        if atlas == 'language':\n",
    "            voxel = data.values[subject_id][voxel_id]\n",
    "            if np.isnan(voxel):\n",
    "                experiment_voxel_nas[experiment].add(len(subject_voxels))\n",
    "            subject_voxels.append(voxel)\n",
    "\n",
    "    experiment_voxels[experiment].append(subject_voxels)\n",
    "    experiment_subjects[experiment].append(subject)\n",
    "    experiment_stimuli[experiment].append(stimulus_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d0de5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384sentences 384 13553 1398\n",
      "243sentences 243 13553 5522\n",
      "384sentences (384, 12155)\n",
      "243sentences (243, 8031)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# we filter out the voxels that are na\n",
    "\n",
    "for experiment in experiment_voxels:\n",
    "    print(\n",
    "        experiment, \n",
    "        len(experiment_voxels[experiment]), \n",
    "        len(experiment_voxels[experiment][0]), \n",
    "        len(experiment_voxel_nas[experiment])\n",
    "    )\n",
    "    \n",
    "experiments = {}\n",
    "for experiment in experiment_voxels:\n",
    "    experiments[experiment] = np.array(\n",
    "        [\n",
    "            [voxel for voxel_id, voxel in enumerate(voxels) if voxel_id not in experiment_voxel_nas[experiment]]\n",
    "            for voxels in experiment_voxels[experiment]\n",
    "        ]\n",
    "    )\n",
    "    print(experiment, experiments[experiment].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6cecacba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "384sentences-fold0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:24<00:00,  1.87s/it]\n",
      "384sentences-fold1: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:23<00:00,  1.79s/it]\n",
      "384sentences-fold2: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:23<00:00,  1.79s/it]\n",
      "384sentences-fold3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:25<00:00,  1.92s/it]\n",
      "384sentences-fold4: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:21<00:00,  1.68s/it]\n",
      "243sentences-fold0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:06<00:00,  1.96it/s]\n",
      "243sentences-fold1: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:07<00:00,  1.81it/s]\n",
      "243sentences-fold2: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:07<00:00,  1.84it/s]\n",
      "243sentences-fold3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:06<00:00,  1.92it/s]\n",
      "243sentences-fold4: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:07<00:00,  1.85it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# 2 experiments x 5 folds x 13 layers\n",
    "experiment_pearsonrs = defaultdict(lambda: np.zeros((5, 13)))\n",
    "for experiment, brain_reps in experiments.items(): \n",
    "    # splits need to be by stimulus_id (how do we shuffle here?)\n",
    "    # (though really they should be by passage_id given how we're doing the GPT2 encoding...\n",
    "    # otherwise the test set will leak into the train set...)\n",
    "    k_folds = GroupShuffleSplit(n_splits=5, train_size=0.8)\n",
    "\n",
    "    for fold, (train_indices, test_indices) in enumerate(\n",
    "        k_folds.split(brain_reps, groups=experiment_stimuli[experiment])\n",
    "    ):\n",
    "        test_subjects = [\n",
    "            experiment_subjects[experiment][brain_rep_idx] \n",
    "            for brain_rep_idx in test_indices\n",
    "        ]\n",
    "        \n",
    "        train_brain_reps, test_brain_reps = brain_reps[train_indices], brain_reps[test_indices]\n",
    "        for layer_num in tqdm(range(13), desc='%s-fold%s' % (experiment, fold)):\n",
    "            train_hidden_states = np.stack([\n",
    "                activations[experiment_stimuli[experiment][brain_rep_idx]][layer_num].numpy()\n",
    "                for brain_rep_idx in train_indices\n",
    "            ])\n",
    "            test_hidden_states = np.stack([\n",
    "                activations[experiment_stimuli[experiment][brain_rep_idx]][layer_num] .numpy()\n",
    "                for brain_rep_idx in test_indices\n",
    "            ])\n",
    "\n",
    "            # TODO: Are they doing any kind of hyperparameter tuning\n",
    "            # (regularization, etc) here?  We're using SKLearn's defaults\n",
    "            \n",
    "            model = LinearRegression().fit(train_hidden_states, train_brain_reps)\n",
    "            pred_brain_reps = model.predict(test_hidden_states)\n",
    "\n",
    "            # We aggregated voxel/electrode/ROI predictivity scores by taking the\n",
    "            # median of scores for each participant’s voxels/electrodes/ROIs and\n",
    "            # then computing the median across participants. Finally, this score was\n",
    "            # divided by the estimated ceiling value (see below) to yield a final score in\n",
    "            # the range [0, 1].\n",
    "\n",
    "            # https://github.com/brain-score/brain-score/blob/master/brainscore/metrics/xarray_utils.py#L78\n",
    "            # https://github.com/brain-score/brain-score/blob/master/brainscore/metrics/regression.py#L33\n",
    "            # https://github.com/brain-score/brain-score/blob/master/brainscore/metrics/transformations.py#L42\n",
    "\n",
    "            # not totally sure this is right...\n",
    "\n",
    "            layer_pearson_rs_by_subj = defaultdict(list)\n",
    "            for pred_brain_rep, test_brain_rep, test_subject in zip(pred_brain_reps, test_brain_reps, test_subjects):\n",
    "                layer_pearson_rs_by_subj[test_subject].append(pearsonr(pred_brain_rep, test_brain_rep))\n",
    "\n",
    "            experiment_pearsonrs[experiment][fold][layer_num] = np.median([\n",
    "                np.median(subj_pearson_rs) for subj_pearson_rs in layer_pearson_rs_by_subj.values()\n",
    "            ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fefc4fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384sentences\n",
      "(0, 0.0007542024393813523, 9.990087661635725e-06)\n",
      "(1, 0.0017777627476372741, 2.357027559063973e-09)\n",
      "(2, 0.008793941305723538, 1.3007383400174443e-06)\n",
      "(3, 0.013204335414826036, 0.003345693082923446)\n",
      "(4, 0.0002499079852198603, 0.00011448259210339833)\n",
      "(5, 0.0027065503204595953, 4.4438247550250133e-07)\n",
      "(6, 0.0008908926065907989, 0.0006890723884412221)\n",
      "(7, 0.0015121320130124338, 0.000723096937342519)\n",
      "(8, 0.0031632659345197907, 1.8181763954417475e-05)\n",
      "(9, 0.004871656723277321, 0.0013079726432238597)\n",
      "(10, 0.00040889134113173797, 2.5757334633372933e-06)\n",
      "(11, 0.005160606227962996, 4.883990555615461e-15)\n",
      "(12, 0.0076998286128112745, 2.422599302678769e-06)\n",
      "243sentences\n",
      "(0, 0.001581102759864113, 1.1841863145481332e-08)\n",
      "(1, 0.010051317046319164, 0.006394502371637348)\n",
      "(2, 0.009191029002548591, 0.011375808695666824)\n",
      "(3, 0.026980557036389907, 0.029602002921695573)\n",
      "(4, 0.008841013505484187, 9.916468992196217e-05)\n",
      "(5, 0.026080265066930574, 0.021168201102157207)\n",
      "(6, 0.006399673583976173, 0.0036177132258204047)\n",
      "(7, 0.03105892628009956, 0.03785184130877647)\n",
      "(8, 0.04003851651379632, 0.0328196285927022)\n",
      "(9, 0.01916963770706921, 0.018516543539382667)\n",
      "(10, 0.03226329585238751, 0.023811718663107686)\n",
      "(11, 0.014640307561865198, 0.0009696676323608553)\n",
      "(12, 0.03046015525155561, 0.033774702994038105)\n"
     ]
    }
   ],
   "source": [
    "for experiment, pearsonrs in experiment_pearsonrs.items():\n",
    "    print(experiment)\n",
    "    for layer_num in range(pearsonrs.shape[1]):\n",
    "        print((layer_num, np.mean(pearsonrs[:, layer_num]), np.median(pearsonrs[:, layer_num])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "475537b0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '►';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '▼';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2 {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.Score (aggregation: 3)&gt;\n",
       "array([0.31856696, 0.01295766, 0.01657364])\n",
       "Coordinates:\n",
       "  * aggregation  (aggregation) object &#x27;center&#x27; &#x27;error_low&#x27; &#x27;error_high&#x27;\n",
       "Attributes:\n",
       "    class_module:  brainscore.metrics\n",
       "    class_name:    Score</pre><div class='xr-wrap' hidden><div class='xr-header'><div class='xr-obj-type'>xarray.Score</div><div class='xr-array-name'></div><ul class='xr-dim-list'><li><span class='xr-has-index'>aggregation</span>: 3</li></ul></div><ul class='xr-sections'><li class='xr-section-item'><div class='xr-array-wrap'><input id='section-8efdb68e-d81f-46c6-882c-1356dd4f050f' class='xr-array-in' type='checkbox' checked><label for='section-8efdb68e-d81f-46c6-882c-1356dd4f050f' title='Show/hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-array-preview xr-preview'><span>0.3186 0.01296 0.01657</span></div><div class='xr-array-data'><pre>array([0.31856696, 0.01295766, 0.01657364])</pre></div></div></li><li class='xr-section-item'><input id='section-6d7393a8-49f5-4dff-b095-492d5a9e89f6' class='xr-section-summary-in' type='checkbox'  checked><label for='section-6d7393a8-49f5-4dff-b095-492d5a9e89f6' class='xr-section-summary' >Coordinates: <span>(1)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>aggregation</span></div><div class='xr-var-dims'>(aggregation)</div><div class='xr-var-dtype'>object</div><div class='xr-var-preview xr-preview'>&#x27;center&#x27; &#x27;error_low&#x27; &#x27;error_high&#x27;</div><input id='attrs-c3dce480-b806-40a8-871d-632d1301b807' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-c3dce480-b806-40a8-871d-632d1301b807' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-def49831-219a-4154-b417-6918f1f87b34' class='xr-var-data-in' type='checkbox'><label for='data-def49831-219a-4154-b417-6918f1f87b34' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([&#x27;center&#x27;, &#x27;error_low&#x27;, &#x27;error_high&#x27;], dtype=object)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-742090e7-bb7a-4ef3-8598-950c90acf1d2' class='xr-section-summary-in' type='checkbox'  checked><label for='section-742090e7-bb7a-4ef3-8598-950c90acf1d2' class='xr-section-summary' >Attributes: <span>(2)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'><dt><span>class_module :</span></dt><dd>brainscore.metrics</dd><dt><span>class_name :</span></dt><dd>Score</dd></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.Score (aggregation: 3)>\n",
       "array([0.31856696, 0.01295766, 0.01657364])\n",
       "Coordinates:\n",
       "  * aggregation  (aggregation) object 'center' 'error_low' 'error_high'\n",
       "Attributes:\n",
       "    class_module:  brainscore.metrics\n",
       "    class_name:    Score"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare with the ceiling\n",
    "pereira.ceiling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
